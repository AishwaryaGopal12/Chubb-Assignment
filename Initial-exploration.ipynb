{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "29e6fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "b09b541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = pd.read_parquet(\"claims.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "eaedfc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = pd.read_excel(\"policies.xlsx\",engine = \"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "78a231d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/miniconda3/lib/python3.8/site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in /opt/miniconda3/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "6322b63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f976a747    4\n",
       "1edad92c    4\n",
       "206daa37    4\n",
       "03d18350    4\n",
       "bd8d5692    4\n",
       "           ..\n",
       "c0c31cc6    1\n",
       "66e20e0c    1\n",
       "5df4372c    1\n",
       "5d3eca51    1\n",
       "7908e09d    1\n",
       "Name: pol, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policies['pol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "250d4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(parquet_file, csv_file, xlsx_file):\n",
    "    claims = pd.read_parquet(parquet_file)\n",
    "    policies = pd.read_excel(xlsx_file, engine = \"openpyxl\")\n",
    "    properties = pd.read_csv(csv_file)\n",
    "    claims_clean = date_process(claims, \"start_date\")\n",
    "    policies_start_clean = date_process(policies, \"start\")\n",
    "    policies_clean = date_process(policies_start_clean, \"end\")\n",
    "    final_data = data_process(claims_clean, policies_clean, properties)\n",
    "    print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "e7f180d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_process(df, column):\n",
    "    df1 = df[column].str.extract('(?P<day>\\d+)(?P<month>[A-Za-z]{3})(?P<year>.*)')\n",
    "    df1['year'].replace(regex=True, inplace=True, to_replace=r'[^0-9]', value=r'')\n",
    "    df2 = pd.concat([df, df1], axis=1)\n",
    "    df2.drop(column, axis = 1, inplace = True)\n",
    "    df2[column] = pd.to_datetime(df2['year'].astype(str)  + df2['month'] + df2['day'].astype(str), format='%Y%b%d', errors = 'coerce')\n",
    "    df2.drop(['day', 'month', 'year'], axis = 1, inplace = True)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "75597dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(claims, policies, properties):\n",
    "    claims.dropna(inplace = True)\n",
    "    policies.dropna(inplace = True)\n",
    "    policies['duration'] = policies['end'] - policies['start']\n",
    "    policies['duration'] = (policies['duration']/ datetime.timedelta(days = 365)).astype(int)\n",
    "    claims_grouped = claims.groupby(['property', 'pol', 'start_date']).count().reset_index()\n",
    "    interim_result = pd.merge(claims_grouped, properties, left_on = \"property\", right_on = \"prop_id\", how = \"inner\")\n",
    "    interim_result.drop(['pol_x', 'property'], inplace = True, axis = 1)\n",
    "    interim_result.rename(columns = {'pol_y': 'pol', 'amount': 'claims'}, inplace = True)\n",
    "    result = pd.merge(interim_result, policies, left_on = ['pol', 'start_date'], right_on = ['pol', 'start'], how = \"inner\")\n",
    "    result.drop(['end', 'start', 'prop_id'], inplace = True, axis = 1)\n",
    "    result['exposure'] = (result['sqft']/1000) * result['duration']\n",
    "    result['claim_frequency'] = result['claims']/result['exposure']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "9c7bfe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      start_date  claims       pol state   sqft  age  duration  exposure  \\\n",
      "0     2015-11-06       5  ae71135b    AZ  88972   83         1    88.972   \n",
      "1     2015-11-06       1  ae71135b    OH  43434   11         1    43.434   \n",
      "2     2015-11-06       8  ae71135b    FL  66035    8         1    66.035   \n",
      "3     2015-11-06       5  ae71135b    AZ  18145   59         1    18.145   \n",
      "4     2015-11-06       3  ae71135b    AZ  74404   50         1    74.404   \n",
      "...          ...     ...       ...   ...    ...  ...       ...       ...   \n",
      "14237 2018-07-06      11  2cde1518    OH  65374   39         1    65.374   \n",
      "14238 2018-07-06       1  2cde1518    AZ  84879   45         1    84.879   \n",
      "14239 2018-07-06       6  2cde1518    AZ  65986   25         1    65.986   \n",
      "14240 2018-07-06      10  2cde1518    OH  16746   85         1    16.746   \n",
      "14241 2018-07-06       3  2cde1518    AZ  24166   47         1    24.166   \n",
      "\n",
      "       claim_frequency  \n",
      "0             0.056197  \n",
      "1             0.023023  \n",
      "2             0.121148  \n",
      "3             0.275558  \n",
      "4             0.040320  \n",
      "...                ...  \n",
      "14237         0.168263  \n",
      "14238         0.011781  \n",
      "14239         0.090928  \n",
      "14240         0.597158  \n",
      "14241         0.124141  \n",
      "\n",
      "[14242 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "read_files(\"claims.parquet\", \"properties.csv\", \"policies.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "614e136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(result_final, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "62241967",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps = [\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, ['sqft', 'age', 'exposure', 'claims_no']),\n",
    "        (\"cat\", categorical_transformer, ['state']),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", Ridge(alpha = 1e-6)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "191d69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = result_final.drop(columns = ['claim_frequency'])\n",
    "y_train = result_final['claim_frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "6afb8f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'mean_absolute_error' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mget_scorer\u001b[0;34m(scoring)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSCORERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mean_absolute_error'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-423-8ea59ed8ce7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m pd.DataFrame(cross_validate(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_absolute_error\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    428\u001b[0m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;31m# Heuristic to ensure user has not passed a metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mget_scorer\u001b[0;34m(scoring)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSCORERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             raise ValueError('%r is not a valid scoring value. '\n\u001b[0m\u001b[1;32m    388\u001b[0m                              \u001b[0;34m'Use sorted(sklearn.metrics.SCORERS.keys()) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                              'to get valid options.' % scoring)\n",
      "\u001b[0;31mValueError\u001b[0m: 'mean_absolute_error' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pd.DataFrame(cross_validate(\n",
    "    pipe, X_train, y_train, return_train_score=True, \n",
    "    scoring=\"mean_absolute_error\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e1d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
