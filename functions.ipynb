{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e5e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.linear_model import PoissonRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2fda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(parquet_file, csv_file, xlsx_file):\n",
    "    '''\n",
    "    Reading data from different file paths and some pre-processing\n",
    "    \n",
    "    Parameters:\n",
    "    -----------------------------\n",
    "        parquet_file: str, path to the parquet file\n",
    "        csv_file: str, path to the csv file\n",
    "        xlsx_file: str, path to the xlsx file\n",
    "        \n",
    "    Returns:\n",
    "    -----------------------------\n",
    "        a dataframe that contains the final data to be fed to the model\n",
    "    '''\n",
    "    \n",
    "    # Reading the parquet file\n",
    "    claims = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Reading the excel file\n",
    "    policies = pd.read_excel(xlsx_file, engine = \"openpyxl\")\n",
    "    \n",
    "    # Reading the csv file\n",
    "    properties = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Call to date process function to process the date column\n",
    "    claims_clean = date_process(claims, \"start_date\")\n",
    "    policies_start_clean = date_process(policies, \"start\")\n",
    "    policies_clean = date_process(policies_start_clean, \"end\")\n",
    "    \n",
    "    # Call to data process function to join dataframes and produce final result\n",
    "    final_data = data_process(claims_clean, policies_clean, properties)\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00a5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_process(df, column):\n",
    "    '''\n",
    "    Process the date column and convert it into a datetime object\n",
    "    \n",
    "    Parameters:\n",
    "    -----------------------------\n",
    "        df: dataframe, the dataframe that contains columns to be processed\n",
    "        column: str, column to be processed\n",
    "        \n",
    "    Returns:\n",
    "    -----------------------------\n",
    "        a dataframe with the specified columns processed\n",
    "    '''\n",
    "    \n",
    "    # Regex to extract day, month and year into separate dataframe from the date column\n",
    "    df1 = df[column].str.extract('(?P<day>\\d+)(?P<month>[A-Za-z]{3})(?P<year>.*)')\n",
    "    \n",
    "    # Remove any special characters present in the year column\n",
    "    df1['year'].replace(regex=True, inplace=True, to_replace=r'[^0-9]', value=r'')\n",
    "    \n",
    "    # Join the original dataframe with the dataframe containing day, month and year\n",
    "    df2 = pd.concat([df, df1], axis=1)\n",
    "    \n",
    "    # Drop the raw date column\n",
    "    df2.drop(column, axis = 1, inplace = True)\n",
    "    \n",
    "    # Combining the day, month and year into a datetime object and dropping the columns\n",
    "    df2[column] = pd.to_datetime(df2['year'].astype(str)  + df2['month'] + df2['day'].astype(str), format='%Y%b%d', errors = 'coerce')\n",
    "    df2.drop(['day', 'month', 'year'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57044710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(claims, policies, properties):\n",
    "    \n",
    "    '''\n",
    "    Reading data from different dataframes and some pre-processing\n",
    "    \n",
    "    Parameters:\n",
    "    -----------------------------\n",
    "        claims: dataframe, the dataframe that contains claims data\n",
    "        policies: dataframe, the dataframe that contains policies data\n",
    "        properties: dataframe, the dataframe that contains properties data\n",
    "        \n",
    "    Returns:\n",
    "    -----------------------------\n",
    "        a dataframe with the specified columns processed\n",
    "    '''\n",
    "    \n",
    "    # Removing the NA values. It's very few values (5-10)\n",
    "    claims.dropna(inplace = True)\n",
    "    policies.dropna(inplace = True)\n",
    "    \n",
    "    # Calculate the policy duration and convert it into years\n",
    "    policies['duration'] = policies['end'] - policies['start']\n",
    "    policies['duration'] = (policies['duration']/ datetime.timedelta(days = 365)).astype(int)\n",
    "    \n",
    "    # Group claims to get the number of claims associated with each property and policy\n",
    "    claims_grouped = claims.groupby(['property', 'pol', 'start_date']).count().reset_index()\n",
    "    \n",
    "    # Merge the grouped claims dataset and properties to obtain the attributes of the property.\n",
    "    interim_result = pd.merge(claims_grouped, properties, left_on = \"property\", right_on = \"prop_id\", how = \"inner\")\n",
    "    \n",
    "    # Drop some columns and rename a few other\n",
    "    interim_result.drop(['pol_x', 'property'], inplace = True, axis = 1)\n",
    "    interim_result.rename(columns = {'pol_y': 'pol', 'amount': 'claims'}, inplace = True)\n",
    "    \n",
    "    # merge the resultant dataframe with the policies to obtain the term of each policy\n",
    "    result = pd.merge(interim_result, policies, left_on = ['pol', 'start_date'], right_on = ['pol', 'start'], how = \"inner\")\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    result.drop(['end', 'start', 'prop_id', 'start_date', 'pol'], inplace = True, axis = 1)\n",
    "    \n",
    "    # Calculate exposure and claims_frequency\n",
    "    result['exposure'] = (result['sqft']/1000) * result['duration']\n",
    "    result['claim_frequency'] = result['claims']/result['exposure']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064d717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = read_files(\"claims.parquet\", \"properties.csv\", \"policies.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbafa179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(result_final):\n",
    "    \n",
    "    '''\n",
    "    Training the final model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------------------------\n",
    "        result_final: dataframe, the dataframe to be fed to the model\n",
    "        \n",
    "    Returns:\n",
    "    -----------------------------\n",
    "        the final trained model\n",
    "    '''\n",
    "    \n",
    "    # Split dataset into regressors and target\n",
    "    X_train = result_final.drop(columns = ['claim_frequency'])\n",
    "    y_train = result_final['claim_frequency']\n",
    "    \n",
    "    # Define preprocessing steps required\n",
    "    # Scale the numeric variables \n",
    "    numeric_transformer = Pipeline(\n",
    "    steps = [\n",
    "        (\"scaler\", StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Transform the categorical variables using One-hot encoding\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Defining a Column Transformer to combine the numeric and categorical variables pre-processing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, ['sqft', 'age', 'exposure', 'claims']),\n",
    "            (\"cat\", categorical_transformer, ['state']),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"regressor\", PoissonRegressor(alpha = 1e-3, max_iter=300))\n",
    "        ]\n",
    "    ).fit(X_train, y_train, regressor__sample_weight = X_train['claims'])\n",
    "    \n",
    "    return pipe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e09685",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_train(result_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99766487",
   "metadata": {},
   "source": [
    "## The below part describes the model selection process. The train function only has the final selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd6ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(result_final, test_size=0.2, random_state=0)\n",
    "    \n",
    "X_train = df_train.drop(columns = ['claim_frequency'])\n",
    "y_train = df_train['claim_frequency']\n",
    "\n",
    "X_test = df_test.drop(columns = ['claim_frequency'])\n",
    "y_test = df_test['claim_frequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601829f",
   "metadata": {},
   "source": [
    "### My initial instinct was to try a linear regression model. So I decided to try the Linear model with regularization. On training I realised that negative values were a possibility and that was not desirable in this scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "3d79e831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_mean_absolute_error</th>\n",
       "      <th>train_neg_mean_absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026948</td>\n",
       "      <td>0.021511</td>\n",
       "      <td>-0.108987</td>\n",
       "      <td>-0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028242</td>\n",
       "      <td>0.009153</td>\n",
       "      <td>-0.120165</td>\n",
       "      <td>-0.111731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.008371</td>\n",
       "      <td>-0.114362</td>\n",
       "      <td>-0.116266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>-0.115167</td>\n",
       "      <td>-0.116023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022605</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>-0.116796</td>\n",
       "      <td>-0.112906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_mean_absolute_error  \\\n",
       "0  0.026948    0.021511                     -0.108987   \n",
       "1  0.028242    0.009153                     -0.120165   \n",
       "2  0.022598    0.008371                     -0.114362   \n",
       "3  0.023083    0.008475                     -0.115167   \n",
       "4  0.022605    0.008435                     -0.116796   \n",
       "\n",
       "   train_neg_mean_absolute_error  \n",
       "0                      -0.118300  \n",
       "1                      -0.111731  \n",
       "2                      -0.116266  \n",
       "3                      -0.116023  \n",
       "4                      -0.112906  "
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pd.DataFrame(cross_validate(\n",
    "    pipe, X_train, y_train, return_train_score=True, \n",
    "    scoring=[\"neg_mean_absolute_error\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "ab9ecc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "X_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ccf0b3",
   "metadata": {},
   "source": [
    "#### Then I went through the GLM family of models. Gamma seemed to be the best fit because it had a range of 0 to inf. The reasoning being that claims_frequency could be 0 o greater and is a continuous variable. Most of the other models seemed to have a discrete range and this was not on par with the expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "7f15f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/statsmodels/genmod/generalized_linear_model.py:293: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n",
      "  warnings.warn((f\"The {type(family.link).__name__} link function \"\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "gamma_model = sm.GLM(y_train, X_transformed, family=sm.families.Gamma())\n",
    "gamma_results = gamma_model.fit()\n",
    "\n",
    "gr = gamma_results.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "ee858e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14210745, 0.03227437, 0.19305008, ..., 0.24241986, 0.22168222,\n",
       "       0.08315073])"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "04d1b84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fad88d1f5e0>"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(gr, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766663a",
   "metadata": {},
   "source": [
    "#### But the results were bad. So, I decided to try the Poisson Regression class which ouput a continuous range and I also had the option to place extra weight on the number of claims and decide the claim frequency. That is the model that is finally a part of the model training function. Also, I decided against using a ZIF model because when joining the tables, i seemed to have an issue joining the 3 and most of the properties without claims was excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58248253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chubb]",
   "language": "python",
   "name": "conda-env-chubb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
